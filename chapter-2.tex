%%
%% VERSION HISTORY
%%    22 May 2006 - John Papandriopoulos - Original version
%%    12 Jul 2007 - John Papandriopoulos - Converted into template
%%

\chapter{Literature review}
	\label{chapter:a-new-hope}
	%

% preferred location for figures in this chapter
\setfigurepath{figures/chapter-2}

%=========================================================================

\begin{synopsis}
	In this chapter...
\end{synopsis}

%=========================================================================

\section{Emotion classification in suicide notes}
Although the research of suicidal text started in 1960s with linguistic analysis of suicide notes to distinguish between genuine and stimulated notes, the application of natural language processing  and machine learning in the same task appear in 2010 \cite{Desmet2013}. Pestian et al \cite{Pestian2010} applied ML algorithms to differentiate the notes of suicide completer and notes written by healthy control group and compared the accuracy of classifiers to accuracy of mental health providers. The best algorithm obtained the accuracy of 78\%, exceeding human professionals accuracy by 13\%. This promising result laid the foundation of NLP techniques on suicide notes. It is also suggest the potential possibility of computer assist human in assessing suicide risk. However, the samsple size for the study is only 66 and is derived from old study which reduce the reliability since the machine focus on the structure of the notes rather than content like what human usually do. Previously, Huang et al \cite{Huang2008} collected blog entries from MySpace.com and use dictionaries containing suicide-related keywords to detect blogs with suicidal intention. The study is one of the first to use social media data to find suicide thoughts and that, in turn, set the new goal for NLP in the mental research: identifying mental suffering victims actively using social network and are at brink of suicide. Unfortunately, MySpace lost its popularity lead to researchers switching to other social networking sites for their study and finding the keyword is a primitive approach for a NLP task.\\

In 2011, the competition i2b2 NLP Challenge (https://www.i2b2.org/NLP/Coreference/) attracted 106 scientists from 24 teams to solve a shared task \cite{Pestian2012}. Track 2 of the challenge require participants to classify sentences in suicide notes in one or more of 15 categories which are 13 emotions such as love, guilt and 2 nonemotion class namely instruction and information. The training set is 600 actual suicide notes collected by Dr. Edwin Shneidman and Cincinnati Children Hospital Medical Center and annotated by different reviewers. Each team is allowed to submit at most three systems thus some teams have similar strategies: develop two classifiers and "combine" them into a hybrid classfier. Wang et al \cite{Wang2012} developed a ML classifier using support vector machine (SVM) along with "one-against-one" approach for multi-label classification. Before training the data, they pre-processed data to aim for betting parser accuracy and give similar meaning to syntactically different expression. For example, they correct misspellings, normalize symbol and number (’+’ to ’and’, ’\$100’ to ’\$MONEY\$’). Their second system is a rule-based classifier that create set of patterns from training data automatically and using $\chi^2$ test and choose a fixed number of pattern set to reduce pattern space. A notion called g-measure was defined, it is a conditional probability of sentence that contain a specific pattern belong to a specific class. The highest g-measure is chosen for that categories to be labelled for a sentence if it is higher than a specific threshold. The team has experimented to find threshold that produce highest F-score and choose it to be fixed threshold for hybrid system. A simple combination rule is used to form a hybrid classifier: if a sentence is not labelled by SVM classifier, then result of rule-based classifier is assigned to that sentence. The threshold approach give better precision (often higher than 0.6) than recall (lower than 0.4) since there are some sentences with various emotions in them and the threshold overlook low occurence emotion.\\

Similarly, Sohn et al \cite{Sohn2012} proposed a ML system and a rule emphasis system. They used Weka (Waikato Environment for Knowledge Analysis \footnote{https://www.cs.waikato.ac.nz/ml/weka/}) and its multinomial Naive Bayes to create the ML system. Some techniques are applied in the process namely token normalization (e.g. "can’t", "can n’t" convert to "cannot"), classifier ensemble (using different values for a parameter) and corpus-reannotation. Their rule-based system simply utilize Perl regularexpressions to match defined patterns. Again, hybrid system combine the result of the two systems to get the final label. They have more balanced result between precision and recall with both are in range 0.5 to 0.6. They also have data pre-processing with popular methods like symbol normalization using regular expression and post-processing phase. A shortcoming of this paper is lack of deep syntactic analysis making some rare classes (sorrow, forgiveness, abuse) did not occur at all. Another improvement can be made by analyze the effect of combining two systems, why it only improve the F-score just a tiny bit compared to other systems.\\

Luyckx et al \cite{Luyckx2012} approached the problem with a new way as they pre-process the data then carry out two processing phases: calibration and thresholding. In the pre-processing step, they tried to divided multi-labelled sentences into single-labelled fragments. If the sentence cannot be divided, then it would be discarded from training data. The calibration phase use SVM classifier with ten-fold cross-validation scheme on output of pre-processing step and evaluate lexical features, context features and lexicon-based features. The lexical features includes some frequently occurred word that associated with a specific emotion (e.g. "can’t", "tired" for hopelessness). The context features consider the label of preceding and following sentences while lexicon-based features make use of a vocabulary of emotion. The thresholding phase applied various threshold to LibSVM (a learning package) probability estimates. In term of F-score, out of these three group, Union system of Sohn’s group attained highest score of 0.5640 followed by 0.5038 and 0.5018 of Wang’s team and Luyckx’s team respectively. The mean performance of all team is 0.4875 with the median of 0.5027. The team with highest score achieve F-measure of 0.6139.\\

\iffalse
  The differences between these systems are summarized in Table 1.\\
\fi
Although all three teams have done some pre-processing steps but only Sohn’s team do post-processing part to eliminate misclassified cases such as salutation being labelled instruction. They also used different learning algorithm from other teams (Naive Bayes vs SVM). This can be one of the reasons why they have smaller difference between precision and recall compared to their counterpart. Naive Bayes is usually work well on small tex collection. Unfortunately, Luyckx’s team could have achieved better result if they did not re-annotate the data. They chose to re-annotate because it show promising result in the development phase but it turn out not good as they expect with F-score of 0.5018 while intact data could give them 0.5230 with roughly the same precision and recall (0.53 and 0.5 respectively). In these papers, the teams use stadard, well-known methods in NLP and off-the-shelf packages to implement their systems, and it is not surprise since this challenge aim is to apply techniques to problem, not investigating pure theory.



